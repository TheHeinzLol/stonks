{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2087e3a-c4b7-4ac6-966e-293fc1ba51ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install dotenv\n",
    "pip install minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3c4ba01-3070-4c44-b3e0-b1dec348181c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are dummy. They were changed long before this notebook got near the internet, so they are safe to show. \n",
    "credentials = {\n",
    "        \"key\": \"PKCTYL4MO5SA2QEIZL2TDEJRTA\",\n",
    "        \"secret_key\": \"DKL2eLVYFcxKzMJtDYbuG3zppWjpwHzsTS1DtHVwc9Cz\" \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d1d62e-21b5-4e6a-a68e-4b37c477f379",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d7c4831-47b2-4dd4-be64-51406a4ab196",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "\n",
    "from datetime import datetime, timedelta, timezone, UTC\n",
    "from pathlib import Path\n",
    "from typing import Union, Tuple, Optional\n",
    "from urllib.parse import urlencode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d8135f4a-bad6-4d26-8cd1-7a58f27372fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used in:\n",
    "    #get_most_active_stocks\n",
    "    #get_top_movers\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"APCA-API-KEY-ID\": credentials[\"key\"],\n",
    "    \"APCA-API-SECRET-KEY\": credentials[\"secret_key\"]\n",
    "}\n",
    "\n",
    "def get_tickers() -> None:\n",
    "    # get S&P 500 tickers\n",
    "    # no arguments, returns nothing, creates json list with tickers\n",
    "    from bs4 import BeautifulSoup\n",
    "    \n",
    "    response = requests.get('https://stockanalysis.com/list/sp-500-stocks/')\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find all <td> elements with the specific class\n",
    "    td_elements = soup.find_all('td', class_='sym svelte-1ro3niy')\n",
    "    \n",
    "    # Extract text from <a> tags inside those <td> elements\n",
    "    tickers = []\n",
    "    for td in td_elements:\n",
    "        a_tag = td.find('a')\n",
    "        if a_tag and a_tag.text.strip():\n",
    "            tickers.append(a_tag.text.strip())\n",
    "            \n",
    "    # Write those into json file\n",
    "    with open(\"tickers.json\", \"w\") as json_file:\n",
    "        json.dump(tickers, json_file)\n",
    "\n",
    "\n",
    "# get top active stocks\n",
    "def get_most_active_stocks(by: str = \"volume\", top: int = 100) -> str:\n",
    "    \"\"\"Calls API and returns pretty formated json string of most active stocks at the moment of last update. \n",
    "    Args:\n",
    "        by: either 'volume' or 'trades'\n",
    "        top: a number between 1 and 100\"\"\"\n",
    "    \n",
    "    url = f\"https://data.alpaca.markets/v1beta1/screener/stocks/most-actives?by={by}&top={top}\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    return response.text\n",
    "\n",
    "\n",
    "# get top market movers\n",
    "def get_top_movers(top: int = 100) -> str:\n",
    "    \"\"\"Calls API and returns pretty formated json string of top market movers, both gainers and losers, at the moment of last update. \n",
    "    Unfortunately, it doesn't filter out penny stocks, so we can't look at stocks worth, say, $5 and more.\n",
    "    Args:\n",
    "        top: a number between 1 and 50. How many of each gainers and losers to return.\"\"\"\n",
    "    \n",
    "    url = f\"https://data.alpaca.markets/v1beta1/screener/stocks/movers?top={top}\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    return response.text\n",
    "\n",
    "# Used in validate_arguments\n",
    "def validate_time_frame(time_frame: str) -> Union[bool, str]:\n",
    "    \"\"\"\n",
    "    Validate if time_frame satisfies the aggregation format:\n",
    "    - [1-59]Min or [1-59]T for minutes\n",
    "    - [1-23]Hour or [1-23]H for hours  \n",
    "    - 1Day or 1D for days\n",
    "    - 1Week or 1W for weeks\n",
    "    - [1,2,3,4,6,12]Month or [1,2,3,4,6,12]M for months\n",
    "    \"\"\"\n",
    "    # ^(\\d+) - Capture digits at start\n",
    "    # (Min|T|Hour|H|Day|D|Week|W|Month|M)$ - Specific suffixes\n",
    "    match = re.match(r'^(\\d+)(Min|T|Hour|H|Day|D|Week|W|Month|M)$', str(time_frame))\n",
    "    \n",
    "    if not match:\n",
    "        return False, \"Invalid format. Must be: [number][unit]\"\n",
    "    \n",
    "    value = int(match.group(1))\n",
    "    unit = match.group(2)\n",
    "    \n",
    "    # Validate based on unit\n",
    "    if unit in ['Min', 'T']:\n",
    "        if not (1 <= value <= 59):\n",
    "            return False, \"Minutes must be between 1-59\"\n",
    "            \n",
    "    elif unit in ['Hour', 'H']:\n",
    "        if not (1 <= value <= 23):\n",
    "            return False, \"Hours must be between 1-23\"\n",
    "            \n",
    "    elif unit in ['Day', 'D']:\n",
    "        if value != 1:\n",
    "            return False, \"Days must be exactly 1\"\n",
    "            \n",
    "    elif unit in ['Week', 'W']:\n",
    "        if value != 1:\n",
    "            return False, \"Weeks must be exactly 1\"\n",
    "            \n",
    "    elif unit in ['Month', 'M']:\n",
    "        valid_months = [1, 2, 3, 4, 6, 12]\n",
    "        if value not in valid_months:\n",
    "            return False, f\"Months must be one of {valid_months}\"\n",
    "    \n",
    "    return True, None\n",
    "\n",
    "# Used in validate_arguments\n",
    "def validate_datetime(date_string: str) -> Union[str, str]:\n",
    "    logging.basicConfig(filename='datetime.log', level=logging.ERROR)\n",
    "    \n",
    "    if not isinstance(date_string, str):\n",
    "        logging.info((f\"date_string is of type '{type(date_string)}'\"))\n",
    "        return None, \"Input must be a string\"\n",
    "    \n",
    "    # Pattern for YYYY-MM-DD\n",
    "    yyyy_mm_dd_pattern = r'^\\d{4}-\\d{2}-\\d{2}$'\n",
    "    # Pattern for YYYY-MM-DDThh:mm:ss (with optional timezone)\n",
    "    datetime_pattern = r'^\\d{4}-\\d{2}-\\d{2}[Tt ]\\d{2}:\\d{2}:\\d{2}'\n",
    "    \n",
    "    date_string = date_string.strip()\n",
    "    try:\n",
    "        if re.match(yyyy_mm_dd_pattern, date_string):\n",
    "            logging.info((\"yyyy_mm_dd matched\"))\n",
    "            date_string = str(datetime.strptime(date_string, \"%Y-%m-%d\").isoformat())\n",
    "            logging.info((f\"Before: {date_string}\\nAfter: {date_string}\\n-------------------------------\"))\n",
    "            return date_string, None\n",
    "        elif re.match(datetime_pattern, date_string):\n",
    "            logging.info((\"datetime matched\"))\n",
    "            # Replace space with T\n",
    "            if ' ' in date_string:\n",
    "                date_string = date_string.replace(' ', 'T')\n",
    "            # Handle different timezone cases\n",
    "            if date_string.upper().endswith('Z'):\n",
    "                # Already has Z timezone\n",
    "                date_string = str(datetime.fromisoformat(date_string.replace('Z', '+00:00')).isoformat())\n",
    "                logging.info((f\"Before: {date_string}\\nAfter: {date_string}\\n-------------------------------\"))\n",
    "                return date_string, None\n",
    "            elif '+' in date_string or '-' in date_string[10:]:\n",
    "                # Has timezone offset\n",
    "                date_string = str(datetime.fromisoformat(date_string).isoformat())\n",
    "                logging.info((f\"Before: {date_string}\\nAfter: {date_string}\\n-------------------------------\"))\n",
    "                return date_string, None\n",
    "            else:\n",
    "                date_string = date_string[:19]\n",
    "                date_string = str(datetime.fromisoformat(date_string).replace(tzinfo=timezone.utc).isoformat())\n",
    "                logging.info((f\"Before: {date_string}\\nAfter: {date_string}\\n-------------------------------\"))\n",
    "                return date_string, None\n",
    "        else:\n",
    "           return date_string, (\"Unsupported date format. Use YYYY-MM-DD, YYYY-MM-DDThh:mm:ss or ime expressed in RFC-3339 format\")\n",
    "            \n",
    "    except ValueError as e:\n",
    "        return date_string, (f\"Invalid date: {str(e)}\")\n",
    "    except Exception as e:\n",
    "        return date_string, (f\"Error processing date: {str(e)}\")\n",
    "\n",
    "\n",
    "# Used in get_historical_bars\n",
    "def validate_arguments(\n",
    "    tickers_to_search: list[str],\n",
    "    date_start: str,\n",
    "    date_end: str,\n",
    "    time_frame: str = \"1D\",\n",
    "    limit: int = 10000,) -> Tuple[bool, list[str]]:\n",
    "    \"\"\" \n",
    "        This function validate arguments before passing those into API call.\n",
    "        Args:\n",
    "            tickers_to_search:\n",
    "                non-empty list of stock tickers.\n",
    "            time_frame:\n",
    "                [1-59]Min or [1-59]T, e.g. 5Min or 5T creates 5-minute aggregations.\n",
    "                [1-23]Hour or [1-23]H, e.g. 12Hour or 12H creates 12-hour aggregations.\n",
    "                1Day or 1D creates 1-day aggregations.\n",
    "                1Week or 1W creates 1-week aggregations.\n",
    "                [1,2,3,4,6,12]Month or [1,2,3,4,6,12]M, e.g. 3Month or 3M creates 3-month aggregations.\n",
    "            date_start and date-end:\n",
    "                string in YYYY-MM-DD or rfc-3339 format. date_start should be an earlier date than date_end.\n",
    "            limit: \n",
    "                number between 1 and 10000.\n",
    "        Returns:\n",
    "            Tuple of (is_valid, error_messages)\n",
    "                \"\"\"\n",
    "        \n",
    "    error_messages = []\n",
    "    # Validate tickers list\n",
    "    if len(tickers_to_search) < 1:\n",
    "        error_messages.append(\"'tickers_to_search' must be a non-emplty list of tickers\")\n",
    "        \n",
    "    # Validate limit\n",
    "    if not (1 <= limit <= 10000):\n",
    "        error_messages.append(f\"'limit' must be between 1 and 10000. Your input: {limit}\")\n",
    "        \n",
    "    # Validate timeframe\n",
    "    is_time_frame_valid, error_message = validate_time_frame(time_frame)\n",
    "    if not is_time_frame_valid:\n",
    "        error_messages.append(f\"{error_message}. Your input: {time_frame}\")\n",
    "        \n",
    "    # Validate dates\n",
    "    dates_legit = True\n",
    "    for date in [date_start, date_end]:\n",
    "        date, error_message = validate_datetime(date)\n",
    "        if error_message is not None:\n",
    "            error_messages.append(f\"{error_message}. Your input: {date}\")\n",
    "            dates_legit = False\n",
    "            \n",
    "    # If dates are valid, check if order is valid, too. \n",
    "    if dates_legit == True:\n",
    "        if datetime.fromisoformat(date_start) >= datetime.fromisoformat(date_end):\n",
    "            error_messages.append(f\"date_start should be an earlier date than date_end. Your date_start: {date_start} and date_end: {date_end}\")\n",
    "\n",
    "    return len(error_messages) == 0, error_messages\n",
    "\n",
    "\n",
    "# Used in get_historical_bars\n",
    "def write_into_json_file(data: str,\n",
    "                         dates: Tuple[str, str],\n",
    "                         i: int = 0,\n",
    "                         path: Path = Path.cwd()) -> None:\n",
    "    first_ticker = list(data['bars'].keys())[0]\n",
    "    first_date = data['bars'][first_ticker][0]['t']\n",
    "    last_ticker = list(data['bars'].keys())[-1]\n",
    "    last_date = data['bars'][first_ticker][-1]['t']\n",
    "    file_name = f\"{first_ticker}_{first_date}-{last_ticker}_{last_date}.json\"\n",
    "    path = Path.cwd().parent / \"data\" / file_name\n",
    "    with open(path, \"w\") as json_file:\n",
    "        json.dump(data, json_file)\n",
    "\n",
    "\n",
    "def get_historical_bars(\n",
    "    tickers_to_search: list[str],\n",
    "    time_frame: str = \"1D\",\n",
    "    date_start: str = None,\n",
    "    date_end: str = None,\n",
    "    limit: int = 10000,) -> bool:\n",
    "    \"\"\" \n",
    "        Saves json in file or prints errors made in arguments.\n",
    "        Args:\n",
    "            tickers_to_search:\n",
    "                non-empty list of stock tickers.\n",
    "            time_frame:\n",
    "                [1-59]Min or [1-59]T, e.g. 5Min or 5T creates 5-minute aggregations.\n",
    "                [1-23]Hour or [1-23]H, e.g. 12Hour or 12H creates 12-hour aggregations.\n",
    "                1Day or 1D creates 1-day aggregations.\n",
    "                1Week or 1W creates 1-week aggregations.\n",
    "                [1,2,3,4,6,12]Month or [1,2,3,4,6,12]M, e.g. 3Month or 3M creates 3-month aggregations.\n",
    "            date_start and date-end:\n",
    "                string in YYYY-MM-DD or rfc-3339 format. date_start should be an earlier date than date_end.\n",
    "            limit: \n",
    "                number between 1 and 10000.\n",
    "        Returns:\n",
    "            bool. \n",
    "                \"\"\"\n",
    "        \n",
    "    # Set defaults properly (evaluated at call time)\n",
    "    if date_start is None:\n",
    "        date_start = (datetime.now().date() - timedelta(days=1)).isoformat()\n",
    "    if date_end is None:\n",
    "        date_end = datetime.now().date().isoformat()\n",
    "        \n",
    "    # Validate arguments\n",
    "    is_valid, error_messages = validate_arguments(tickers_to_search, date_start, date_end, time_frame, limit)\n",
    "    if not is_valid:\n",
    "        for error in error_messages:\n",
    "            print(error)\n",
    "        return False\n",
    "            \n",
    "    params = {\n",
    "    'symbols': \",\".join(tickers_to_search),\n",
    "    'timeframe': time_frame, \n",
    "    'start': date_start,\n",
    "    'end': date_end,\n",
    "    'limit': limit,\n",
    "    'adjustment': 'raw',\n",
    "    'feed': 'sip', \n",
    "    'page_token': \"\",\n",
    "    'sort': \"asc\"\n",
    "    }\n",
    "    while params['page_token'] is not None: # while there is next page\n",
    "        url = f\"https://data.alpaca.markets/v2/stocks/bars?{urlencode(params)}\"\n",
    "        response = requests.get(url, headers=headers)\n",
    "        parsed = json.loads(response.text)\n",
    "        write_into_json_file(parsed, (date_start, date_end))\n",
    "        params['page_token'] = parsed['next_page_token']\n",
    "        time.sleep(7)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f79f39b-6115-4f5b-bff6-939288ed08c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_strings = [\n",
    "        # Valid YYYY-MM-DD\n",
    "        \"2023-12-25\",\n",
    "        \"2000-01-01\",\n",
    "        \"1999-12-31\",\n",
    "        \n",
    "        # Valid RFC-3339\n",
    "        \"2023-12-25T10:30:45Z\",\n",
    "        \"2023-12-25T10:30:45+00:00\",\n",
    "        \"2023-12-25T10:30:45-05:00\",\n",
    "        \"2023-12-25T10:30:45.123Z\",\n",
    "        \"2023-12-25T10:30:45.123456+02:00\",\n",
    "        \"2025-11-28T09:48:08.075452\", # Missing timezone\n",
    "        \"2023-12-25T10:30:45\",  # Missing timezone\n",
    "        \n",
    "        # Invalid cases\n",
    "        \"2023-13-25\",  # Invalid month\n",
    "        \"2023-12-32\",  # Invalid day\n",
    "        \"2023-13-35\", # Both day and month are invalid\n",
    "        \"2023/12/25\",  # Wrong separator\n",
    "        \"25-12-2023\",  # Wrong order\n",
    "        \"2023-12-25T25:30:45Z\",  # Invalid hour\n",
    "        \"not-a-date\",\n",
    "        \"2023-12-25T10:30:45+5:00\",  # Timezone without leading zero\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9835b308-70ab-4e14-a931-a45b9c387256",
   "metadata": {},
   "source": [
    "## airflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d61f17cd-e548-4174-870c-aa1faff3305c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "\n",
    "from minio import Minio\n",
    "from minio.error import S3Error\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5832f725-a63a-4ba0-9343-7967145eefe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "admin admin_password\n"
     ]
    }
   ],
   "source": [
    "def get_minio_credentials(path_to_env_file: Union[str, Path] = \"../config/minio.env\") -> str:\n",
    "    \"\"\"This function returns login and password for root user of minio server, getting those from 'minio.env' file.\n",
    "    'minio.env' should have MINIO_ROOT_USER and MINIO_ROOT_PASSWORD variables. If there are no such variables, asks user to provide those via input.\n",
    "    Args:\n",
    "        path_to_env_file: either string or pathlib.Path object leading to minio.env file.\"\"\"\n",
    "    load_dotenv(path_to_env_file)\n",
    "    MINIO_ROOT_USER = os.getenv(\"MINIO_ROOT_USER\")\n",
    "    MINIO_ROOT_PASSWORD = os.getenv(\"MINIO_ROOT_PASSWORD\")\n",
    "    if (MINIO_ROOT_USER and MINIO_ROOT_PASSWORD):\n",
    "        return MINIO_ROOT_USER, MINIO_ROOT_PASSWORD \n",
    "    else:\n",
    "        print(f\"There are no MINIO_ROOT_USER and/or MINIO_ROOT_PASSWORD variables in {path_to_env_file}\")\n",
    "\n",
    "login, password = get_minio_credentials()\n",
    "print(login, password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ce590709-49a5-4ecd-add1-f7e9ce4373cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a client\n",
    "# args: login, password\n",
    "host = \"localhost\"\n",
    "client = Minio(\n",
    "    endpoint=f\"{host}:9000\",\n",
    "    access_key=login,\n",
    "    secret_key=password,\n",
    "    secure=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "1b548e0e-0cd1-4a2a-b7c4-3d78ac8eb2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a bucket\n",
    "# args: \n",
    "    #client:Minio\n",
    "bucket_name = \"bucketass\"\n",
    "\n",
    "def create_bucket(bucket_name: str, client: Minio) -> None:\n",
    "    found = client.bucket_exists(bucket_name=bucket_name)\n",
    "    if not found:\n",
    "        client.make_bucket(bucket_name=bucket_name)\n",
    "        print(\"Created bucket\", bucket_name)\n",
    "    else:\n",
    "        print(\"Bucket\", bucket_name, \"already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e2d56241-f110-4e18-a939-7472a22969af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created bucket bucketass\n"
     ]
    }
   ],
   "source": [
    "create_bucket(bucket_name=bucket_name, client=client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "954d3171-129c-416d-8b48-a1344b8fe8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "source_file = \"test.txt\" # str or path\n",
    "#make two fput and put for string and stream uploads\n",
    "def upload_to_bucket(source_file: Union[str, Path, bytes], bucket_name: str, client: Minio, destination_file: str=None) -> None:\n",
    "    # Make the bucket if it doesn't exist.\n",
    "    create_bucket(bucket_name, client)\n",
    "    # Check if name of file has to be changed before upload:\n",
    "    if destination_file is None:\n",
    "        destination_file = Path(source_file).name\n",
    "    # upload file\n",
    "    client.fput_object(\n",
    "        bucket_name=bucket_name,\n",
    "        object_name=destination_file,\n",
    "        file_path=source_file\n",
    "    )\n",
    "    print(\n",
    "        \"successfully uploaded object\", destination_file, \"to bucket\", bucket_name,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "20085708-98d2-4ed6-80d7-f5609b851a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket bucketass already exists\n",
      "successfully uploaded object test.txt to bucket bucketass\n"
     ]
    }
   ],
   "source": [
    "upload_to_bucket(source_file, bucket_name, client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff32b12-0889-4df6-b3d8-b3f069bed6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tutor \n",
    "import textwrap\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Operators; we need this to operate!\n",
    "from airflow.providers.standard.operators.bash import BashOperator, PythonOperator\n",
    "\n",
    "# The DAG object; we'll need this to instantiate a DAG\n",
    "from airflow.sdk import DAG\n",
    "\n",
    "def simple_print():\n",
    "    print(\"It is a simple print\")\n",
    "    \n",
    "with DAG(\n",
    "    \"tutorial\",\n",
    "    # These args will get passed on to each operator\n",
    "    # You can override them on a per-task basis during operator initialization\n",
    "    default_args={\n",
    "        \"depends_on_past\": False,\n",
    "        \"retries\": 1,\n",
    "        \"retry_delay\": timedelta(seconds=5),\n",
    "        # 'queue': 'bash_queue',\n",
    "        # 'pool': 'backfill',\n",
    "        # 'priority_weight': 10,\n",
    "        # 'end_date': datetime(2016, 1, 1),\n",
    "        # 'wait_for_downstream': False,\n",
    "        # 'execution_timeout': timedelta(seconds=300),\n",
    "        # 'on_failure_callback': some_function, # or list of functions\n",
    "        # 'on_success_callback': some_other_function, # or list of functions\n",
    "        # 'on_retry_callback': another_function, # or list of functions\n",
    "        # 'sla_miss_callback': yet_another_function, # or list of functions\n",
    "        # 'on_skipped_callback': another_function, #or list of functions\n",
    "        # 'trigger_rule': 'all_success'\n",
    "    },\n",
    "    description=\"A simple tutorial DAG\",\n",
    "    schedule=timedelta(seconds=1),\n",
    "    start_date=datetime(2021, 1, 1),\n",
    "    catchup=False,\n",
    "    tags=[\"example\"],\n",
    ") as dag:\n",
    "\n",
    "    # t1, t2 and t3 are examples of tasks created by instantiating operators\n",
    "    t1 = BashOperator(\n",
    "        task_id=\"print_date\",\n",
    "        bash_command=\"date\",\n",
    "    )\n",
    "\n",
    "    t2 = BashOperator(\n",
    "        task_id=\"sleep\",\n",
    "        depends_on_past=False,\n",
    "        bash_command=\"sleep 5\",\n",
    "        retries=3,\n",
    "    )\n",
    "    t1.doc_md = textwrap.dedent(\n",
    "        \"\"\"\\\n",
    "    #### Task Documentation\n",
    "    You can document your task using the attributes `doc_md` (markdown),\n",
    "    `doc` (plain text), `doc_rst`, `doc_json`, `doc_yaml` which gets\n",
    "    rendered in the UI's Task Instance Details page.\n",
    "    ![img](https://imgs.xkcd.com/comics/fixing_problems.png)\n",
    "    **Image Credit:** Randall Munroe, [XKCD](https://xkcd.com/license.html)\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    dag.doc_md = __doc__  # providing that you have a docstring at the beginning of the DAG; OR\n",
    "    dag.doc_md = \"\"\"\n",
    "    This is a documentation placed anywhere\n",
    "    \"\"\"  # otherwise, type it like this\n",
    "    templated_command = textwrap.dedent(\n",
    "        \"\"\"\n",
    "    {% for i in range(5) %}\n",
    "        echo \"{{ ds }}\"\n",
    "        echo \"{{ macros.ds_add(ds, 7)}}\"\n",
    "    {% endfor %}\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    t3 = BashOperator(\n",
    "        task_id=\"templated\",\n",
    "        depends_on_past=False,\n",
    "        bash_command=templated_command,\n",
    "    )\n",
    "\n",
    "    t1 >> [t2, t3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4c86d59b-bda5-4f44-855f-66b99d935ca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(datetime.date(2025, 10, 22),\n",
       " datetime.datetime(2025, 11, 1, 12, 32, 10, 970827),\n",
       " '2025-10-22',\n",
       " '2025-11-01T12:32:10.970827')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_start = datetime.now().date() - timedelta(days=41)\n",
    "date_end = datetime.now() - timedelta(days=31)\n",
    "date_start_str = date_start.isoformat()\n",
    "date_end_str = date_end.isoformat()\n",
    "date_start, date_end, date_start_str, date_end_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cd38fb1a-5141-47b4-aeec-2f0339f0ab59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NVDA', 'AAPL']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tickers_to_search = tickers[:2]\n",
    "tickers_to_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0f5b56f2-3ade-4e3c-9a71-ac2f25464bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we can only get 15 minutes delayed data, we lag date_end 15 minutes back from now if date_end is today.\n",
    "if date_end.date() == datetime.now().date():\n",
    "    date_end = datetime.now().replace(microsecond=0) - timedelta(minutes=15) \n",
    "\n",
    "# We have rate limit for 200 api calls per minute and 10000 records mer call. So we can only gather 11 days of historical data in one minute,\n",
    "# which allows us to make one call every 6-7 seconds to not exceed rate limit.\n",
    "date_start = date_start - timedelta(days=1) # offset for while loop to not step over date_end\n",
    "while date_start < date_end.date():\n",
    "    date_start = date_start + timedelta(days=1)\n",
    "    #if we stepped over date_end, clamp both dates within the day of date_end\n",
    "    if date_start > date_end.date(): \n",
    "        date_start_str = date_end.date().isoformat()\n",
    "        date_end_str = date_end.isoformat()\n",
    "    # if we did not, then gather 11 days from date_start\n",
    "    else:\n",
    "        date_start_str = date_start.isoformat()\n",
    "        date_end_str = (date_start + timedelta(days=11)).isoformat()\n",
    "        \n",
    "    get_historical_bars(tickers_to_search, date_start=date_start_str, date_end=date_end_str, time_frame=\"1Min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7f28821-f2d0-4d56-a1d3-9d2112c32105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2020, 12, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.now().date() - timedelta(days=5*365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "472fabd8-71da-4f49-a9e6-a018f70c1e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_to_search = [\"NVDA\", \"AAPL\"]\n",
    "time_frame = \"1Min\"\n",
    "date_start = \"2025-11-10\"\n",
    "date_end = \"2025-11-25\"\n",
    "limit = 10000\n",
    "\n",
    "params = {\n",
    "    'symbols': \",\".join(tickers_to_search),\n",
    "    'timeframe': time_frame, \n",
    "    'start': date_start,\n",
    "    'end': date_end,\n",
    "    'limit': limit,\n",
    "    'adjustment': 'raw',\n",
    "    'feed': 'sip', \n",
    "    'page_token': \"\",\n",
    "    'sort': \"asc\"\n",
    "    }\n",
    "i = 0\n",
    "    #while params['page_token'] is not None: # while there is next page\n",
    "url = f\"https://data.alpaca.markets/v2/stocks/bars?{urlencode(params)}\"\n",
    "response = requests.get(url, headers=headers)\n",
    "parsed = json.loads(response.text)\n",
    "params['page_token'] = parsed['next_page_token']\n",
    "i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1b905e75-2476-445b-a050-aff940818573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVDA'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('tickers.json', \"r\") as tickers_file:\n",
    "    tickers = json.load(tickers_file)\n",
    "tickers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326c08bd-33f1-460e-a5a2-99d309f7c4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_start = datetime.now(UTC).replace(microsecond=0, second=0, minute=0, hour=0) - timedelta(days=365*5, hours=5)\n",
    "date_end = datetime.now(UTC).replace(microsecond=0) - timedelta(hours=5, minutes=15)\n",
    "get_historical_bars(tickers_to_search = tickers_to_search,\n",
    "time_frame = \"1Min\",\n",
    "date_start = date_start.isoformat(),\n",
    "date_end = date_end.isoformat(),\n",
    "limit = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9ba15757-94f2-47d5-a7f3-22212e18031f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_to_search = tickers.copy()\n",
    "tickers_to_search.remove('AAPL')\n",
    "tickers_to_search.remove('NVDA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7f391e75-6be3-4e27-b254-a71637cdb991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "501"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tickers_to_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72b8ce3-f21a-4886-8197-328a265e0515",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abc37548-c1d7-4ed0-b1cf-ac7edd7d34db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from pyspark.sql import SparkSession\n",
    "from typing import Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2603797-bf11-4c2f-aa3d-aa16a684029f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "26/01/13 15:52:15 WARN Utils: Your hostname, ubuntu-home, resolves to a loopback address: 127.0.1.1; using 192.168.0.29 instead (on interface enp3s0)\n",
      "26/01/13 15:52:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/13 15:52:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "def get_minio_credentials(path_to_env_file: Union[str, Path] = \"../config/minio.env\") -> str:\n",
    "    \"\"\"This function returns login and password for root user of minio server, getting those from 'minio.env' file.\n",
    "    'minio.env' should have MINIO_ROOT_USER and MINIO_ROOT_PASSWORD variables. If there are no such variables, asks user to provide those via input.\n",
    "    Args:\n",
    "        path_to_env_file: either string or pathlib.Path object leading to minio.env file.\"\"\"\n",
    "    load_dotenv(path_to_env_file)\n",
    "    MINIO_ROOT_USER = os.getenv(\"MINIO_ROOT_USER\")\n",
    "    MINIO_ROOT_PASSWORD = os.getenv(\"MINIO_ROOT_PASSWORD\")\n",
    "    if (MINIO_ROOT_USER and MINIO_ROOT_PASSWORD):\n",
    "        return MINIO_ROOT_USER, MINIO_ROOT_PASSWORD \n",
    "    else:\n",
    "        print(f\"There are no MINIO_ROOT_USER and/or MINIO_ROOT_PASSWORD variables in {path_to_env_file}\")\n",
    "\n",
    "login_minio, password_minio = get_minio_credentials()\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"bronze_to_silver\")\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", login_minio)\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", password_minio)\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    #.config(\"spark.driver.extraJavaOptions\", \"-Djava.security.manager\") # to solve JDK 23+ compatibility problem\n",
    "    .getOrCreate()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07160096-d48b-4ed7-b0eb-14d49b58f4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark._jvm.org.apache.hadoop.util.VersionInfo.getVersion()\n",
    "\n",
    "spark: 3.4.1\n",
    "hadoop: 3.3.4\n",
    "aws-java-sdk-bundle: 1.12.262\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "login_minio, password_minio = \"admin\", \"admin_password\"\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder \n",
    "    .appName(\"bronze_to_silver\") \n",
    "    .master(\"spark://spark-master:7077\") \n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4\") \n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", login_minio) \n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", password_minio) \n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \n",
    "            \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \n",
    "    .getOrCreate()\n",
    "        )\n",
    "\n",
    "s3_file_path = \"s3a://airflow.learn/2026-01-13 08:37:19.193383+00:00\"\n",
    "\n",
    "bronze_df = spark.read\\\n",
    "    .format(\"binaryFile\")\\\n",
    "    .load(s3_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba53e6f-ba70-4723-855f-871b50ac576d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "login_minio, password_minio = \"admin\", \"admin_password\"\n",
    "jars = \"jars/hadoop-aws-3.4.2.jar,jars/bundle-2.29.52.jar\"\n",
    "spark = (\n",
    "    SparkSession.builder \n",
    "    .appName(\"bronze_to_silver\") \n",
    "    .config(\"spark.jars\", jars) \n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", login_minio) \n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", password_minio) \n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \n",
    "            \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \n",
    "    .getOrCreate()\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2bde008-4934-46ae-814b-46afdb98821e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark==3.5.8\n",
      "  Downloading pyspark-3.5.8.tar.gz (317.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.8/317.8 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m  \u001b[33m0:01:05\u001b[0m[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:02\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: py4j<0.10.9.10,>=0.10.9.7 in /home/heinz/miniconda3/envs/stonks/lib/python3.12/site-packages (from pyspark==3.5.8) (0.10.9.9)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.8-py2.py3-none-any.whl size=318353002 sha256=691fbe262e75fd0f2edf8b319c39fa83a9fbd46283a6f70be2353e9e6f621b46\n",
      "  Stored in directory: /home/heinz/.cache/pip/wheels/f0/f6/86/a9231691706c40d5bcc8c907f583e0ef90c075dcfa97e272d0\n",
      "Successfully built pyspark\n",
      "Installing collected packages: pyspark\n",
      "Successfully installed pyspark-3.5.8\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark==3.5.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d3adc2f-e411-4c55-935d-642ea61c9a91",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[32m      3\u001b[39m spark = (\n\u001b[32m      4\u001b[39m     \u001b[43mSparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mremote\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msc://localhost:15002\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m )\n\u001b[32m      9\u001b[39m spark.range(\u001b[32m10\u001b[39m).show()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/stonks/lib/python3.12/site-packages/pyspark/sql/session.py:479\u001b[39m, in \u001b[36mSparkSession.Builder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    477\u001b[39m     os.environ[\u001b[33m\"\u001b[39m\u001b[33mSPARK_CONNECT_MODE_ENABLED\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    478\u001b[39m     opts[\u001b[33m\"\u001b[39m\u001b[33mspark.remote\u001b[39m\u001b[33m\"\u001b[39m] = url\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mRemoteSparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mopts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mSPARK_LOCAL_REMOTE\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m os.environ:\n\u001b[32m    481\u001b[39m     url = \u001b[33m\"\u001b[39m\u001b[33msc://localhost\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/stonks/lib/python3.12/site-packages/pyspark/sql/connect/session.py:221\u001b[39m, in \u001b[36mSparkSession.Builder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    219\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m session \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    220\u001b[39m         session = \u001b[38;5;28mself\u001b[39m.create()\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply_options\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m session\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/stonks/lib/python3.12/site-packages/pyspark/sql/connect/session.py:183\u001b[39m, in \u001b[36mSparkSession.Builder._apply_options\u001b[39m\u001b[34m(self, session)\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._options.items():\n\u001b[32m    182\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m         \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    185\u001b[39m         warnings.warn(\u001b[38;5;28mstr\u001b[39m(e))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/stonks/lib/python3.12/site-packages/pyspark/sql/connect/conf.py:41\u001b[39m, in \u001b[36mRuntimeConf.set\u001b[39m\u001b[34m(self, key, value)\u001b[39m\n\u001b[32m     39\u001b[39m op_set = proto.ConfigRequest.Set(pairs=[proto.KeyValue(key=key, value=value)])\n\u001b[32m     40\u001b[39m operation = proto.ConfigRequest.Operation(\u001b[38;5;28mset\u001b[39m=op_set)\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m warn \u001b[38;5;129;01min\u001b[39;00m result.warnings:\n\u001b[32m     43\u001b[39m     warnings.warn(warn)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/stonks/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1368\u001b[39m, in \u001b[36mSparkConnectClient.config\u001b[39m\u001b[34m(self, operation)\u001b[39m\n\u001b[32m   1366\u001b[39m req.operation.CopyFrom(operation)\n\u001b[32m   1367\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1368\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mattempt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_retrying\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1369\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mattempt\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1370\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stub\u001b[49m\u001b[43m.\u001b[49m\u001b[43mConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_builder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/stonks/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1684\u001b[39m, in \u001b[36mRetrying.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1681\u001b[39m             backoff += random.uniform(\u001b[32m0\u001b[39m, \u001b[38;5;28mself\u001b[39m._jitter)\n\u001b[32m   1683\u001b[39m         logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRetrying call after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackoff\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ms sleep\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1684\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackoff\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1000.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1685\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m AttemptManager(\u001b[38;5;28mself\u001b[39m._can_retry, retry_state)\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m retry_state.done():\n\u001b[32m   1688\u001b[39m     \u001b[38;5;66;03m# Exceeded number of retries, throw last exception we had\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .remote(\"sc://localhost:15002\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.range(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6367b232-0529-4fe7-a357-65222eb205fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting grpcio-status\n",
      "  Downloading grpcio_status-1.76.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting protobuf<7.0.0,>=6.31.1 (from grpcio-status)\n",
      "  Downloading protobuf-6.33.4-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Collecting grpcio>=1.76.0 (from grpcio-status)\n",
      "  Downloading grpcio-1.76.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: googleapis-common-protos>=1.5.5 in /home/heinz/miniconda3/envs/stonks/lib/python3.12/site-packages (from grpcio-status) (1.72.0)\n",
      "Requirement already satisfied: typing-extensions~=4.12 in /home/heinz/miniconda3/envs/stonks/lib/python3.12/site-packages (from grpcio>=1.76.0->grpcio-status) (4.15.0)\n",
      "Downloading grpcio_status-1.76.0-py3-none-any.whl (14 kB)\n",
      "Downloading protobuf-6.33.4-cp39-abi3-manylinux2014_x86_64.whl (323 kB)\n",
      "Downloading grpcio-1.76.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: protobuf, grpcio, grpcio-status\n",
      "\u001b[2K  Attempting uninstall: protobuf\n",
      "\u001b[2K    Found existing installation: protobuf 4.25.8\n",
      "\u001b[2K    Uninstalling protobuf-4.25.8:\n",
      "\u001b[2K      Successfully uninstalled protobuf-4.25.8\n",
      "\u001b[2K  Attempting uninstall: grpcio\n",
      "\u001b[2K    Found existing installation: grpcio 1.65.5\n",
      "\u001b[2K    Uninstalling grpcio-1.65.5:\n",
      "\u001b[2K      Successfully uninstalled grpcio-1.65.5\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [grpcio-status]m \u001b[32m1/3\u001b[0m [grpcio]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "opentelemetry-proto 1.27.0 requires protobuf<5.0,>=3.19, but you have protobuf 6.33.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed grpcio-1.76.0 grpcio-status-1.76.0 protobuf-6.33.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "spark._jsc.hadoopConfiguration().get(\"fs.s3a.endpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5cfe96-1830-40b5-bc2d-a9f352f6f360",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.format(\"binaryFile\") \\\n",
    "    .load(\"s3a://airflow.learning/\") \\\n",
    "    .limit(5) \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc18bc3-81af-4a2a-bfc6-5a348774f8de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
